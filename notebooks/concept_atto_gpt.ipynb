{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWoXsNu8CyJeUZSVTSNjBB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debarshee2004/atto-gpt/blob/master/notebooks/concept_atto_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atto GPT (How GPT works?)"
      ],
      "metadata": {
        "id": "J5tcZNXLxU9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's see what dataset we are using for this recreation process.**"
      ],
      "metadata": {
        "id": "8o0A6FuxxzaK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sopgjONBxPCZ",
        "outputId": "d27855f4-da21-4e88-fa72-d1dc9478990b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-11 08:51:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-10-11 08:51:42 (24.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "SGslIPGVxgeP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDVYFQ8KxyLp",
        "outputId": "454ef5df-5e33-4192-b5a0-c21e625ce326"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU8Z4tYjyBXc",
        "outputId": "eb7e7c0d-ea2f-4bb0-8f35-e5301f3a12d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "vnzLZu2fyeaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the dataset"
      ],
      "metadata": {
        "id": "qSCRd9qQyaN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"The Characters in the Dataset:\",''.join(chars))\n",
        "print(\"The Number of Characters in the Dataset:\",vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z18b-zlVyLV5",
        "outputId": "b54e6de6-7bcc-4893-9960-df63c16a0a71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Characters in the Dataset: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "The Number of Characters in the Dataset: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder and Decoder"
      ],
      "metadata": {
        "id": "TfYSMMJM0Kkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "# encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "# decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(\"Encoded Values: `hii there`\")\n",
        "print(\"Encoded Values: \",encode(\"hii there\"))\n",
        "print(\"Decoded Output: \",decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2115JJ7HyovT",
        "outputId": "bfda49cf-e988-45c3-e851-587d8fc3a424"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Values: `hii there`\n",
            "Encoded Values:  [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "Decoded Output:  hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(\"Shape of the Data: \", data.shape)\n",
        "print(\"Data-Type of the Data\", data.dtype)\n",
        "print(\"The encoded data\")\n",
        "print(data[:1000])\n",
        "# the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2LeRZ510n5J",
        "outputId": "1ae9d9cc-8dbf-4cb0-edf5-902be462b7a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the Data:  torch.Size([1115394])\n",
            "Data-Type of the Data torch.int64\n",
            "The encoded data\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "1otSpBC31hx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spliting the Data"
      ],
      "metadata": {
        "id": "oi_46qQH1nQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data))\n",
        "# 90% of the data will be used for training the model\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "55q8ssia1cub"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhWWBfFG1wvv",
        "outputId": "39d1f134-9401-4ad3-b385-98d0c8979f8e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract a slice from the training data as input (x) with 'block_size' number of elements,\n",
        "# starting from the first element and ending at index block_size-1\n",
        "x = train_data[:block_size]\n",
        "\n",
        "# Extract another slice from the training data as target (y), but shifted by 1 position\n",
        "# relative to 'x'. This will start from index 1 and end at index block_size.\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "# Loop through each time step (t) from 0 to block_size-1\n",
        "for t in range(block_size):\n",
        "\n",
        "    # Extract the context, which is the slice from the beginning of 'x' up to the current time step (t) + 1\n",
        "    # This represents the input sequence of increasing length at each step.\n",
        "    context = x[:t+1]\n",
        "\n",
        "    # The target for the current time step is the t-th element from the 'y' sequence\n",
        "    target = y[t]\n",
        "\n",
        "    # Print the current context (input sequence) and its corresponding target value.\n",
        "    # It shows what the model is expected to predict as the target, given the context.\n",
        "    print(f\"when input is {context} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTErjmyN2GVN",
        "outputId": "e2d29904-66ac-42bf-a425-e8b14b81c976"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "6cNTARvY2RRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility, ensuring that the random numbers generated by torch\n",
        "# are the same every time the code runs.\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Define the batch size, i.e., how many independent sequences (or examples) we want to process in parallel\n",
        "batch_size = 4\n",
        "\n",
        "# Define the block size, i.e., the maximum length of the input context for predictions\n",
        "block_size = 8\n",
        "\n",
        "# Function to get a batch of data for training or validation\n",
        "def get_batch(split):\n",
        "    # Select either the training data or validation data based on the 'split' argument\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Randomly choose starting indices for the batch, ensuring that the selected index\n",
        "    # plus the block size does not exceed the data length. This ensures we can create\n",
        "    # sequences of length 'block_size'.\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # Stack 'batch_size' sequences of length 'block_size' from the selected data\n",
        "    # These will serve as the input sequences (x) for the model.\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "\n",
        "    # Stack 'batch_size' sequences for the targets (y), shifted by 1 position relative to 'x'\n",
        "    # The target sequence is what the model will learn to predict.\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "    # Return the input (x) and target (y) tensors for this batch\n",
        "    return x, y\n",
        "\n",
        "# Get a batch of training data (inputs and targets)\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "# Print the shape and contents of the input tensor\n",
        "print('inputs:')\n",
        "print(xb.shape)  # Should show (batch_size, block_size), e.g., (4, 8)\n",
        "print(xb)\n",
        "\n",
        "# Print the shape and contents of the target tensor\n",
        "print('targets:')\n",
        "print(yb.shape)  # Should also show (batch_size, block_size), e.g., (4, 8)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "# Iterate over the batch dimension (each independent sequence in the batch)\n",
        "for b in range(batch_size):\n",
        "    # Iterate over the time dimension (each timestep in the sequence)\n",
        "    for t in range(block_size):\n",
        "        # Extract the context, which is the input sequence from timestep 0 to t\n",
        "        # This represents the portion of the input the model has seen so far\n",
        "        context = xb[b, :t+1]\n",
        "\n",
        "        # Extract the target value at the current timestep t\n",
        "        target = yb[b, t]\n",
        "\n",
        "        # Print the current context (input sequence seen so far) and its corresponding target value\n",
        "        # The 'tolist()' function is used to convert the tensor into a list for better readability\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeuKbzpU2JWR",
        "outputId": "cb2279f8-4f67-4eda-c09c-11d2c01fd73d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BaKx9-n2w0n",
        "outputId": "931a94f4-363b-4eca-e79c-cafb5f2c7101"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "i7crqJcJ2_SK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Initialization\n",
        "\n",
        "The `BigramLanguageModel` class inherits from `torch.nn.Module`, making it a PyTorch neural network module. In the `__init__` method, the model initializes a **token embedding table** using `nn.Embedding`. This table maps each token in the vocabulary to a vector of logits representing the likelihood of the next token. The embedding table’s size is `(vocab_size, vocab_size)`, meaning each token is represented by a vector of size equal to the number of tokens in the vocabulary. This architecture is characteristic of a **bigram model**, where the next token is predicted solely based on the current token."
      ],
      "metadata": {
        "id": "_mjS9a3m4Y5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Method\n",
        "\n",
        "The `forward` method computes the logits for the next token in the sequence. The method takes in a batch of token indices `idx` (with shape `(B, T)`, where `B` is the batch size and `T` is the sequence length) and optionally `targets` (which represent the actual next tokens for training). The logits are generated by looking up each token’s embedding from the table. If `targets` are provided, the method computes the **cross-entropy loss**, a standard loss function for classification tasks, between the predicted logits and the true target values. The logits are reshaped from `(B, T, C)` to `(B*T, C)` to match the expected input shape for the loss function, where `C` is the vocabulary size."
      ],
      "metadata": {
        "id": "U6IfKpYh4l4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Generation\n",
        "\n",
        "The `generate` method implements token generation. Given an initial sequence of token indices (`idx`), the method generates additional tokens up to a specified number (`max_new_tokens`). At each step, the model predicts the next token by computing the logits for the last time step, applying a **softmax** function to convert logits into probabilities, and then sampling the next token using `torch.multinomial`. The newly sampled token is appended to the sequence, and this process is repeated for the desired number of new tokens. This allows the model to create sequences of arbitrary length based on the context provided."
      ],
      "metadata": {
        "id": "ik4YLm324pgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Set a fixed random seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Define a class for the Bigram Language Model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    # Constructor method, initializing the model\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Create an embedding table where each token is mapped to a vector of logits\n",
        "        # The table maps each token in the vocabulary to a vector of size (vocab_size,)\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx: (B, T) tensor, where B = batch size, T = sequence length\n",
        "        # targets: (B, T) tensor, containing the target tokens to predict\n",
        "\n",
        "        # Get the logits for the next token using the embedding table\n",
        "        logits = self.token_embedding_table(idx)  # Output shape: (B, T, C), where C = vocab_size\n",
        "\n",
        "        # If targets are provided, calculate the loss\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Flatten logits and targets to calculate loss using cross-entropy\n",
        "            B, T, C = logits.shape  # Unpack batch size (B), time steps (T), and vocab size (C)\n",
        "            logits = logits.view(B*T, C)  # Reshape logits to (B*T, C) for loss computation\n",
        "            targets = targets.view(B*T)   # Reshape targets to (B*T)\n",
        "            loss = F.cross_entropy(logits, targets)  # Compute cross-entropy loss\n",
        "\n",
        "        # Return logits and loss (if targets are given)\n",
        "        return logits, loss\n",
        "\n",
        "    # Function to generate new tokens based on the current context\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx: (B, T) tensor, where B = batch size, T = sequence length\n",
        "        # max_new_tokens: the number of tokens to generate\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Forward pass to get logits (predictions)\n",
        "            logits, loss = self(idx)\n",
        "\n",
        "            # Focus only on the last time step's logits\n",
        "            logits = logits[:, -1, :]  # Shape: (B, C), where C = vocab_size\n",
        "\n",
        "            # Apply softmax to convert logits into probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # Shape: (B, C)\n",
        "\n",
        "            # Sample the next token from the probability distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # Shape: (B, 1)\n",
        "\n",
        "            # Append the sampled token to the current sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Shape: (B, T+1)\n",
        "\n",
        "        # Return the updated sequence, including generated tokens\n",
        "        return idx\n",
        "\n",
        "# Example usage\n",
        "# Assume 'vocab_size' is defined based on the dataset being used\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "\n",
        "# Perform a forward pass with some input data (xb) and targets (yb)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "# Print the shape of the logits (output predictions) and the computed loss\n",
        "print(\"The shape of the output predictions: \",logits.shape)  # Should show (batch_size * sequence_length, vocab_size)\n",
        "print(\"The Cross Entropy Loss: \",loss)  # The cross-entropy loss value\n",
        "\n",
        "# Generate 100 new tokens based on an initial input of a single token (0)\n",
        "generated_sequence = m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)\n",
        "print(\"The Generated Sequesnce: \",generated_sequence[0].tolist())\n",
        "\n",
        "# Decode the generated token indices into text (assuming a 'decode' function exists)\n",
        "# Convert the generated token indices (a tensor) into a list for decoding\n",
        "print(\"The Decoded Generated Sequesnce: \",decode(generated_sequence[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pf-POFY29x9",
        "outputId": "de9cf828-3aeb-460a-8ab6-97d7833bb6c0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the output predictions:  torch.Size([32, 65])\n",
            "The Cross Entropy Loss:  tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "The Generated Sequesnce:  [0, 31, 56, 12, 55, 28, 7, 29, 35, 49, 58, 36, 53, 24, 4, 48, 24, 16, 22, 45, 27, 24, 34, 64, 5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34, 4, 60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63, 39, 53, 8, 55, 44, 64, 57, 3, 37, 57, 3, 64, 18, 7, 61, 6, 11, 43, 17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57, 2, 47, 35, 35, 8, 27, 40, 64, 16, 52, 62, 13, 1, 25, 57, 3, 9]\n",
            "The Decoded Generated Sequesnce:  \n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "gZWYjqT94-ds"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size (number of training samples processed at once)\n",
        "batch_size = 32\n",
        "\n",
        "# Loop over training steps (iterations)\n",
        "for steps in range(10000):  # Increase the number of steps for better results over time\n",
        "\n",
        "    # Sample a batch of input (xb) and target (yb) data from the training set\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Perform a forward pass through the model and compute the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    # Clear the gradients from the previous step to prevent accumulation\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # Backpropagate the loss to compute gradients for the model parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the model parameters using the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "# Print the final loss value after training loop ends\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhTgcWGn53a7",
        "outputId": "9d880a41-3c3d-4209-9285-7e6fd3d82e7e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6319539546966553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The Embeddings: \" ,m.generate(idx = torch.zeros((1, 1),\n",
        "                        dtype=torch.long),\n",
        "                        max_new_tokens=500)[0].tolist())\n",
        "\n",
        "print(\"The Decoded Embeddings: \",decode(m.generate(idx = torch.zeros((1, 1),\n",
        "                        dtype=torch.long),\n",
        "                        max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2BNujzX6Cmp",
        "outputId": "b868a199-55ba-4e49-e73c-ac3d11e514e7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Embeddings:  [0, 20, 43, 50, 1, 53, 1, 54, 54, 43, 12, 1, 51, 53, 57, 47, 57, 47, 45, 46, 43, 56, 42, 43, 1, 61, 11, 1, 57, 1, 58, 57, 43, 56, 43, 1, 41, 53, 44, 1, 53, 59, 40, 56, 43, 1, 54, 50, 1, 50, 53, 58, 46, 43, 57, 0, 0, 58, 1, 56, 43, 1, 47, 41, 43, 5, 31, 39, 58, 1, 58, 46, 43, 56, 41, 53, 56, 48, 59, 58, 1, 52, 58, 46, 43, 58, 6, 1, 58, 1, 43, 63, 1, 58, 1, 58, 10, 0, 0, 14, 17, 16, 35, 46, 47, 52, 53, 56, 58, 46, 43, 12, 1, 50, 1, 42, 1, 51, 1, 58, 46, 58, 46, 39, 52, 58, 46, 43, 43, 1, 42, 47, 41, 53, 53, 1, 58, 1, 39, 56, 42, 8, 1, 51, 1, 58, 43, 1, 45, 53, 56, 58, 46, 39, 1, 43, 56, 58, 46, 47, 43, 57, 46, 43, 1, 35, 39, 56, 53, 56, 1, 56, 47, 57, 57, 58, 46, 63, 1, 40, 47, 57, 1, 50, 50, 50, 1, 61, 50, 53, 1, 57, 51, 39, 58, 53, 44, 43, 1, 57, 58, 43, 1, 54, 53, 59, 56, 1, 63, 53, 57, 43, 1, 54, 54, 56, 58, 6, 1, 53, 57, 58, 46, 43, 57, 58, 46, 43, 10, 0, 35, 13, 26, 32, 39, 50, 42, 1, 21, 44, 53, 59, 1, 52, 1, 46, 53, 61, 39, 41, 43, 57, 6, 1, 58, 1, 57, 43, 52, 42, 43, 1, 44, 43, 1, 46, 53, 53, 1, 61, 53, 56, 1, 58, 46, 39, 52, 5, 57, 58, 1, 46, 47, 57, 53, 59, 56, 43, 0, 13, 52, 42, 8, 0, 53, 56, 43, 43, 1, 57, 43, 58, 1, 43, 7, 53, 44, 53, 47, 52, 8, 0, 14, 39, 52, 42, 43, 1, 46, 1, 52, 43, 6, 1, 39, 50, 53, 59, 52, 45, 1, 39, 56, 47, 52, 39, 52, 43, 1, 40, 43, 1, 58, 46, 39, 56, 8, 0, 21, 18, 53, 47, 50, 43, 1, 46, 39, 56, 47, 52, 1, 40, 56, 45, 56, 43, 0, 31, 43, 57, 1, 57, 1, 39, 41, 43, 58, 63, 1, 40, 43, 56, 1, 60, 39, 52, 45, 56, 39, 52, 42, 1, 53, 42, 1, 21, 44, 53, 1, 47, 52, 53, 1, 47, 57, 1, 58, 46, 43, 1, 15, 53, 59, 41, 59, 56, 43, 39, 57, 53, 52, 53, 56, 1, 14, 17, 17, 16, 1, 57, 58, 39, 52, 58, 43, 51, 47, 42, 57, 1, 39, 58, 46, 47, 58, 46, 53, 58, 46, 43, 39, 58, 46, 1, 42, 47, 41, 53, 56, 43, 52, 41, 47, 57, 54, 43, 56, 1, 56, 56, 50, 42, 43, 1, 44, 56, 47, 60, 47, 41, 43, 52, 6, 1, 39, 58, 47, 45, 47, 52, 53, 61, 52, 45, 46, 39, 51, 43, 42, 1, 46]\n",
            "The Decoded Embeddings:  \n",
            "Tase chit OMe\n",
            "GBETICENUCED r irt mered herereprogo t watalive:\n",
            "BY:\n",
            "\n",
            "s tul h I gaiens t r and\n",
            "TMughierofe thuro w!\n",
            "SAn as,\n",
            "AMinethonameadsasowano sppavep mpl for haizke hitinduntr\n",
            "\n",
            "\n",
            "An'do EOLLAS:\n",
            "Awotorke at bureougouenaddsen tyon mevy,\n",
            "HEnd anatheimam geay renghistice, ts tur y cersathobe r'le wealprcaworise'ssithimenckn\n",
            "QULUSaks, w,\n",
            "Mulico ow LTw t wirove ive fo grtanenemp-asee blledove ro ssothive.\n",
            "KENGHe conar or w, mys dllo heve;\n",
            "Wan. pan plio, t---ap ongn has as f ffinounghand nd ty ris hat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "CGutVBkT7QNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X6JtXjz6hdL",
        "outputId": "0a4ae45c-0608-460b-e696-21080d60dc22"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co5htX6J7Y0N",
        "outputId": "a9d028db-74e4-4de0-b53e-cac4fe6aad24"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "-Jhbl87r7iYM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WPHTHCt7mrG",
        "outputId": "88e23529-7117-4acc-8bb2-3cc4cb230a33"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XrzjdVy7rlG",
        "outputId": "d2437bac-a524-4b99-e6df-05febf829984"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKRmi8HV717t",
        "outputId": "e49352eb-fe08-4376-b20b-ad940dcbbaba"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMrq1Uiu8rwY",
        "outputId": "b8836e67-4eef-4dd0-f19a-bee869ffcc99"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "u5bHEckT8yVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "7Ndyqqq78vE-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gSohjL_ELgr",
        "outputId": "cdc73437-2f97-4162-844a-dbcc0f9aa375"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uylx24MpENvV",
        "outputId": "fd7f7649-34b9-4071-95e4-c22ccb871fdc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F_a8ipTEP06",
        "outputId": "5cba3c97-c38f-468f-bb81-b220c57a984d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN9sOyNwER8l",
        "outputId": "5d6b55f1-cf5c-4e35-a54c-601ce18af26e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)\n",
        "# gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDiXRZbhEVb7",
        "outputId": "fc748e0b-0011-4321-ffcc-22a077110795"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom LayerNorm1d class (replacing BatchNorm1d)\n",
        "class LayerNorm1d:\n",
        "\n",
        "  # Constructor method for initialization\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    # Small constant to prevent division by zero when normalizing\n",
        "    self.eps = eps\n",
        "\n",
        "    # Learnable scale parameter (gamma), initialized to ones\n",
        "    self.gamma = torch.ones(dim)\n",
        "\n",
        "    # Learnable shift parameter (beta), initialized to zeros\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  # Forward pass method to normalize the input\n",
        "  def __call__(self, x):\n",
        "    # Calculate the mean of each example in the batch (along the feature dimension)\n",
        "    xmean = x.mean(1, keepdim=True)  # Shape: (batch_size, 1)\n",
        "\n",
        "    # Calculate the variance of each example in the batch (along the feature dimension)\n",
        "    xvar = x.var(1, keepdim=True)  # Shape: (batch_size, 1)\n",
        "\n",
        "    # Normalize the input by subtracting the mean and dividing by the standard deviation (unit variance)\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "\n",
        "    # Scale by gamma and shift by beta (element-wise operation)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "\n",
        "    # Return the normalized and scaled output\n",
        "    return self.out\n",
        "\n",
        "  # Method to return the learnable parameters (gamma and beta)\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "# Set a manual seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Instantiate the LayerNorm1d module with a feature dimension of 100\n",
        "module = LayerNorm1d(100)\n",
        "\n",
        "# Generate random input data: a batch of 32 samples, each of dimension 100\n",
        "x = torch.randn(32, 100)\n",
        "\n",
        "# Pass the input data through the LayerNorm1d module (forward pass)\n",
        "x = module(x)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "x.shape  # Should output: torch.Size([32, 100])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyKSiMJiEZrr",
        "outputId": "a33814e6-83f0-4f16-9140-d4cea9c0cf62"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tjj-ivQFRGY",
        "outputId": "a102e995-42df-4480-cfc4-ecf913c94966"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KReLW-KFVAW",
        "outputId": "411a0b97-3f76-409f-ef07-d54affedfe75"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHo4LJNyFW8o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}